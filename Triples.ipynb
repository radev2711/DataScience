{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting RDF triples from plain text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inroduction\n",
    "\n",
    "Working on task for creating Linked Open Data using Wikipedia pages led to realization that most of the first sentences of Wikipedia articles often can be distilled to simple statement. For example the descriptor for the Wikipedia itself \"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\" can be read as \"Wikipedia is encyclopedia\" by removing the decorators (adjectives) and leaving nouns (proper and common in the case) and predicate (aux verb). This lead to the hypothesis that RDF triples can be extracted from raw text.\n",
    "\n",
    "This notebook contains the first experiments to test this hypothesis. For this goal the experiments are structured as follows:\n",
    "\n",
    "    - Data collection - three text documents: one fiction, one media (news articles) and one encyclopedic (Wikipedia articles), roughly the same size.\n",
    "    - Data preparation - sentence splitting, tokenization, lemmatization, POS-tagging of the text documents\n",
    "    - Run experiments - using two approaches\n",
    "        * First approach - look for noun -> verb -> noun patterns in sentences\n",
    "        * Second approach - collect all nouns in the text documents. Create a basic predicate set. Loop the noun and predicatesets to generate noun -> verb -> noun patterns\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "### RDF\n",
    "\n",
    "The Resource Description Framework (RDF) is a framework for expressing information about resources. It is a standard model for data interchange on the Web recommended by World Wide Web Consortium (W3C). RDF provides a common framework for expressing information on the Web so it can be exchanged between applications without loss of meaning. This means that the information may be made available to applications other than those for which it was originally created.\n",
    "\n",
    "RDF is used for series of practices including: adding machine-readable information to Web pages, enriching a dataset by linking it to third-party datasets, interlinking API feeds, putting into work the datasets currently published as Linked Data, building distributed social networks, providing a standards-compliant way for exchanging data between databases, interlinking various datasets and enabling cross-dataset queries with the use of SPARQL.\n",
    "\n",
    "RDF uses International Resource Identifier (IRI) as resource identifies.\n",
    "\n",
    "\n",
    "### RDF Triples\n",
    "\n",
    "RDF allows us to make statements about resources. Due their structure of three elements these statements are called triples. The format of these triples is simple and always has the following structure:\n",
    "\n",
    "    <subject> <predicate> <object>\n",
    "\n",
    "For example:\n",
    "\n",
    "    <Bob> <is a> <person>.\n",
    "    <Bob> <is a friend of> <Alice>.\n",
    "    <Bob> <is born on> <the 4th of July 1990>.\n",
    "\n",
    "The RDF statements expresses a relationship between two resources. The subject and the object represent the two resources being related; the predicate represents the nature of their relationship.\n",
    "\n",
    "### the Turtle Language\n",
    "\n",
    "the Terse RDF Triple Language or simply Turtle allows for the textual representations of an RDF graphs. Turtle introduces a number of syntactic shortcuts, such as support for namespace prefixes, lists and shorthands for datatyped literals. This language provides a trade-off between ease of writing, ease of parsing and readability.\n",
    "\n",
    "Turtle examlpe:\n",
    "\n",
    "    @base <http://example.org/> .\n",
    "    @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "    @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "    @prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "    @prefix rel: <http://www.perceive.net/schemas/relationship/> .\n",
    "\n",
    "    <#green-goblin>\n",
    "        rel:enemyOf <#spiderman> ;\n",
    "        a foaf:Person ;    # in the context of the Marvel universe\n",
    "        foaf:name \"Green Goblin\" .\n",
    "\n",
    "    <#spiderman>\n",
    "        rel:enemyOf <#green-goblin> ;\n",
    "        a foaf:Person ;\n",
    "        foaf:name \"Spiderman\", \"Человек-паук\"@ru .\n",
    "\n",
    "This example introduces many of features of the Turtle language: @base and Relative IRI references, @prefix and prefixed names, predicate lists separated by ';', object lists separated by ',', the token a, and literals. Comments may be given after a '#'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install spaCy run \"conda install -c conda-forge spacy\" and then \"python -m spacy download en_core_web_sm\" in the anaconda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "import spacy\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download as nltk_down\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "The data consists of three documents:\n",
    "- fiction text - The Phoenix on the Sword by Robert E. Howard available via [Project Gutenberg of Australia](http://www.gutenberg.net.au/ebooks06/0600811h.html)\n",
    "- media text - 14 articles from Reuters and DW\n",
    "- enciclopedic text - around 30 Wikipedia articles from different domains: Biology, Geography, Physics, Chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that loads our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_file(file):\n",
    "    text = ''\n",
    "    with open(file, 'r') as input_file:\n",
    "        text = input_file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the text documents we can see the number of characters in each document or some regular expressions to measure the size of the documents in words. The word count should be similar for each of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_text = load_input_file('data/conan.txt')\n",
    "fiction_words = re.split(\"\\W+\", fiction_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_text = load_input_file('data/media.txt')\n",
    "media_words = re.split(\"\\W+\", media_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text = load_input_file('data/wiki.txt')\n",
    "wiki_words = re.split(\"\\W+\", wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character counts: fiction - 50253, media - 57780, wiki - 63849\n",
      "RE word counts: fiction - 9186, media - 9598, wiki - 10220\n"
     ]
    }
   ],
   "source": [
    "print(f\"Character counts: fiction - {len(fiction_text)}, media - {len(media_text)}, wiki - {len(wiki_text)}\")\n",
    "print(f\"RE word counts: fiction - {len(fiction_words)}, media - {len(media_words)}, wiki - {len(wiki_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character counts are not very informative and matching with regular expressions is prone to errors. For text processing is better to look for sentences and words. This can be done wit the help of NLTK and spaCy libraries. They can apply the processes of sentence splitting and tokenization (taking a text or sentence and splitting it into individual units called tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_tokens = word_tokenize(fiction_text)\n",
    "media_tokens = word_tokenize(media_text)\n",
    "wiki_tokens = word_tokenize(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK word counts: fiction - 10585, media - 10728, wiki - 11586'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_counts = f\"NLTK word counts: fiction - {len(fiction_tokens)}, media - {len(media_tokens)}, wiki - {len(wiki_tokens)}\"\n",
    "nltk_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the there is a difference. Maybe an arbiter will help. What will spaCy count.\n",
    "First load the model, then make the document and see the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_doc = nlp(fiction_text)\n",
    "media_doc = nlp(media_text)\n",
    "wiki_doc = nlp(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy word counts: fiction - 11041, media - 11206, wiki - 11897\n",
      "NLTK word counts: fiction - 10585, media - 10728, wiki - 11586\n"
     ]
    }
   ],
   "source": [
    "spacy_counts = f\"spaCy word counts: fiction - {len(fiction_doc)}, media - {len(media_doc)}, wiki - {len(wiki_doc)}\"\n",
    "print(spacy_counts)\n",
    "print(nltk_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem that NLTK and spaCy tokenizers are producing different token counts. Lets see the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_result = []\n",
    "for nltk_token in wiki_tokens[:20]:\n",
    "    nltk_result.append(nltk_token)\n",
    "\n",
    "spacy_result = []\n",
    "for spacy_token in wiki_doc[:20]:\n",
    "    spacy_result.append(spacy_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffTurtles', 'are', 'an', 'order', 'of', 'reptiles', 'known', 'as', 'Testudines', ',', 'characterized', 'by', 'a', 'shell', 'developed', 'mainly', 'from', 'their', 'ribs', '.']\n",
      "[﻿Turtles, are, an, order, of, reptiles, known, as, Testudines, ,, characterized, by, a, shell, developed, mainly, from, their, ribs, .]\n"
     ]
    }
   ],
   "source": [
    "print(nltk_result)\n",
    "print(spacy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks fine. Lets try with the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_sents = sent_tokenize(fiction_text)\n",
    "media_sents = sent_tokenize(media_text)\n",
    "wiki_sents = sent_tokenize(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentences = f\"NLTK sentence counts: fiction - {len(fiction_sents)}, media - {len(media_sents)}, wiki - {len(wiki_sents)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_spacy_sent = list(fiction_doc.sents)\n",
    "media_spacy_sent = list(media_doc.sents)\n",
    "wiki_spacy_sent = list(wiki_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentences = f\"spaCy sentence counts: fiction - {len(fiction_spacy_sent)}, media - {len(media_spacy_sent)}, wiki - {len(wiki_spacy_sent)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK sentence counts: fiction - 565, media - 433, wiki - 486\n",
      "spaCy sentence counts: fiction - 527, media - 434, wiki - 485\n"
     ]
    }
   ],
   "source": [
    "print(nltk_sentences)\n",
    "print(spacy_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough. It seems that the splitters disagree most in the fiction document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "For the purposes of the experiments the text will need to be separated into sentences and tokes. Then the tokens will need to be lemmatized and recieve part-of-speech-tags.\n",
    "\n",
    "Lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item and Part-of-speech (POS) tagging is the assignment of part-of-speech tags to words.\n",
    "\n",
    "Trying to measure the size of the documents the separation work is done. NLTK does lemmatization and POS-tagging via separate methods while spaCy does all at doc creation.\n",
    "\n",
    "Lets begin with NLTK lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ivo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_down('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lematize_tokens(tokens):\n",
    "    lemmas = []\n",
    "\n",
    "    for token in tokens:\n",
    "        lemmetized_word = lemmatizer.lemmatize(token)\n",
    "        lemmas.append(lemmetized_word)\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_lemmas = lematize_tokens(fiction_tokens)\n",
    "media_lemmas = lematize_tokens(media_tokens)\n",
    "wiki_lemmas = lematize_tokens(wiki_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_pos = pos_tag(fiction_lemmas)\n",
    "media_pos = pos_tag(media_lemmas)\n",
    "wiki_pos = pos_tag(wiki_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\ufeffTurtles', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('an', 'DT'),\n",
       " ('order', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('reptile', 'NN'),\n",
       " ('known', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('Testudines', 'NNP'),\n",
       " (',', ','),\n",
       " ('characterized', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('shell', 'NN'),\n",
       " ('developed', 'VBN'),\n",
       " ('mainly', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('rib', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_pos[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should do it. Now to proceed with the triple extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Triples\n",
    "\n",
    "### approach 1\n",
    "\n",
    "### approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_extract_triples(pos_tokens):\n",
    "    nltk_triples = []\n",
    "\n",
    "    for idx, tup in enumerate(pos_tokens):\n",
    "        if tup[0] in ['is', 'are', 'have', 'can']:\n",
    "            predicate = tup[0]\n",
    "            subj = ''\n",
    "            obj = ''\n",
    "            \n",
    "            for i in range(idx, -1, -1):\n",
    "                if 'NN' in pos_tokens[i][1]:\n",
    "                    subj = pos_tokens[i][0]\n",
    "                    break\n",
    "            for j in range(idx, len(pos_tokens)):\n",
    "                if 'NN' in pos_tokens[j][1]:\n",
    "                    obj = pos_tokens[j][0]\n",
    "                    break\n",
    "\n",
    "            triple = f\"{subj} > {predicate} > {obj}\"\n",
    "            nltk_triples.append(triple)\n",
    "\n",
    "    return nltk_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_triples_nltk = nltk_extract_triples(fiction_pos)\n",
    "media_triples_nltk = nltk_extract_triples(media_pos)\n",
    "wiki_triples_nltk = nltk_extract_triples(wiki_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffTurtles > are > order',\n",
       " 'turtle > are > group',\n",
       " 'head > are > living',\n",
       " 'terrapin > are > continent',\n",
       " 'shell > are > bone',\n",
       " 'part > is > carapace',\n",
       " 'underside > is > flatter',\n",
       " 'surface > is > scale',\n",
       " 'Turtles > are > temperature',\n",
       " 'environment > are > omnivore',\n",
       " 'turtle > are > reptile',\n",
       " 'Turtles > have > myth',\n",
       " 'specie > are > pet',\n",
       " 'Turtles > have > meat',\n",
       " 'turtle > are > bycatch',\n",
       " 'world > are > result',\n",
       " 'specie > are > extinction',\n",
       " 'hedgehog > is > mammal',\n",
       " 'Erinaceidae > are > specie',\n",
       " 'introduction > are > Australia']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(wiki_triples_nltk))\n",
    "wiki_triples_nltk[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_triples(document):\n",
    "    spacy_triples = []\n",
    "\n",
    "    for token in document:\n",
    "        if token.lemma_ in ['be', 'have', 'can']:\n",
    "            idx = token.i\n",
    "            predicate = token\n",
    "            subj = ''\n",
    "            obj = ''\n",
    "\n",
    "            for i in range(idx, -1, -1):\n",
    "                if document[i].pos_ == \"NOUN\":\n",
    "                    subj = document[i].lemma_\n",
    "                    break\n",
    "            for j in range(idx, len(document)):\n",
    "                if document[j].pos_ == \"NOUN\":\n",
    "                    obj = document[j].lemma_\n",
    "                    break\n",
    "\n",
    "            triple = f\"{subj} > {predicate} > {obj}\"\n",
    "            spacy_triples.append(triple)\n",
    "\n",
    "    return spacy_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_triples_spacy = spacy_extract_triples(fiction_doc)\n",
    "media_triples_spacy = spacy_extract_triples(media_doc)\n",
    "wiki_triples_spacy = spacy_extract_triples(wiki_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffturtle > are > order',\n",
       " 'turtle > are > group',\n",
       " 'head > are > living',\n",
       " 'terrapin > are > continent',\n",
       " 'shell > are > bone',\n",
       " 'part > is > carapace',\n",
       " 'underside > is > plastron',\n",
       " 'surface > is > scale',\n",
       " 'turtle > are > ectotherm',\n",
       " 'environment > are > omnivore',\n",
       " 'turtle > are > reptile',\n",
       " 'turtle > have > myth',\n",
       " 'specie > are > pet',\n",
       " 'turtle > have > meat',\n",
       " 'turtle > been > meat',\n",
       " 'turtle > are > bycatch',\n",
       " 'world > are > result',\n",
       " 'world > being > result',\n",
       " 'specie > are > extinction',\n",
       " 'hedgehog > is > mammal']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(wiki_triples_spacy))\n",
    "wiki_triples_spacy[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "['dupe > have > street', 'desert > have > heart', 'night > have > agent', 'agent > have > face', 'face > have > empire', 'shadow > have > downfall', 'task > is > wit', 'task > are > wit', 'deed > is > people', 'reign > is > people']\n",
      "333\n",
      "['rise > was > age', 'world > was > supreme', 'countenance > was > door', 'hand > was > giant', 'dupe > have > street', 'desert > have > heart', 'desert > been > heart', 'night > have > noble', 'agent > have > face', 'face > have > empire']\n"
     ]
    }
   ],
   "source": [
    "print(len(fiction_triples_nltk))\n",
    "print(fiction_triples_nltk[:10])\n",
    "print(len(fiction_triples_spacy))\n",
    "print(fiction_triples_spacy[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "['government > have > developer', 'move > are > completion', 'sale > are > future', 'sector > is > tightening', 'project > can > developer', 'project > is > developer', 'developer > have > crisis', 'sector > have > capital', 'efficiency > can > credit', 'developer > are > project']\n",
      "440\n",
      "['government > have > developer', 'company > be > move', 'move > are > completion', 'sale > are > future', 'sector > is > tightening', 'government > has > help', 'sector > has > year', 'sector > been > year', \"demand > 's > dilemma\", 'project > can > developer']\n"
     ]
    }
   ],
   "source": [
    "print(len(media_triples_nltk))\n",
    "print(media_triples_nltk[:10])\n",
    "print(len(media_triples_spacy))\n",
    "print(media_triples_spacy[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [RDF](https://www.w3.org/RDF/)\n",
    "2. [RDF Primer](https://www.w3.org/TR/2014/NOTE-rdf11-primer-20140624/)\n",
    "3. [RDF triples](https://www.w3.org/TR/rdf12-n-triples/)\n",
    "4. [Turtle format](https://www.w3.org/TR/rdf12-turtle/)\n",
    "5. [Entity extraction: From unstructured text to DBpedia RDF triples](https://lucris.lub.lu.se/ws/portalfiles/portal/3053000/3191702.pdf)\n",
    "6. [FactCheck: Validating RDF Triples Using Textual Evidence](https://svn.aksw.org/papers/2018/CIKM_FACTCHECK/public.pdf)\n",
    "7. [Harvesting RDF triples](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=05620dd4b5346e5c17f3f4da97efcb18e2fcb7e6)\n",
    "8. [NLTK](https://www.nltk.org/)\n",
    "9. [Install spaCy](https://spacy.io/usage)\n",
    "10. [NLP with spaCy](http://spacy.pythonhumanities.com/intro.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
