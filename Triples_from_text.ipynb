{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting RDF triples from plain text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inroduction\n",
    "\n",
    "Working on task for creating Linked Open Data using Wikipedia pages led to realization that most of the first sentences of Wikipedia articles often can be distilled to simple statement. For example the descriptor for the Wikipedia itself \"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation.\" can be read as \"Wikipedia is encyclopedia\" by removing the decorators (adjectives) and leaving nouns (proper and common in the case) and predicate (aux verb). This lead to the hypothesis that RDF triples can be extracted from raw text.\n",
    "\n",
    "This notebook contains the first experiments to test this hypothesis. For this goal the experiments are structured as follows:\n",
    "\n",
    "    - Data collection - three text documents: one fiction, one media (news articles) and one encyclopedic (Wikipedia articles), roughly the same size.\n",
    "    - Data preparation - sentence splitting, tokenization, lemmatization, POS-tagging of the text documents\n",
    "    - Run experiments - using two approaches\n",
    "        * First approach - look for noun -> verb -> noun patterns in sentences\n",
    "        * Second approach - collect all nouns in the text documents. Create a basic predicate set. Loop the noun and predicatesets to generate noun -> verb -> noun patterns\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "### RDF\n",
    "\n",
    "The Resource Description Framework (RDF) is a framework for expressing information about resources. It is a standard model for data interchange on the Web recommended by World Wide Web Consortium (W3C). RDF provides a common framework for expressing information on the Web so it can be exchanged between applications without loss of meaning. This means that the information may be made available to applications other than those for which it was originally created.\n",
    "\n",
    "RDF is used for series of practices including: adding machine-readable information to Web pages, enriching a dataset by linking it to third-party datasets, interlinking API feeds, putting into work the datasets currently published as Linked Data, building distributed social networks, providing a standards-compliant way for exchanging data between databases, interlinking various datasets and enabling cross-dataset queries with the use of SPARQL.\n",
    "\n",
    "RDF uses International Resource Identifier (IRI) as resource identifies.\n",
    "\n",
    "\n",
    "### RDF Triples\n",
    "\n",
    "RDF allows us to make statements about resources. Due their structure of three elements these statements are called triples. The format of these triples is simple and always has the following structure:\n",
    "\n",
    "    <subject> <predicate> <object>\n",
    "\n",
    "For example:\n",
    "\n",
    "    <Bob> <is a> <person>.\n",
    "    <Bob> <is a friend of> <Alice>.\n",
    "    <Bob> <is born on> <the 4th of July 1990>.\n",
    "\n",
    "The RDF statements expresses a relationship between two resources. The subject and the object represent the two resources being related; the predicate represents the nature of their relationship.\n",
    "\n",
    "### the Turtle Language\n",
    "\n",
    "the Terse RDF Triple Language or simply Turtle allows for the textual representations of an RDF graphs. Turtle introduces a number of syntactic shortcuts, such as support for namespace prefixes, lists and shorthands for datatyped literals. This language provides a trade-off between ease of writing, ease of parsing and readability.\n",
    "\n",
    "Turtle examlpe:\n",
    "\n",
    "    @base <http://example.org/> .\n",
    "    @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "    @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "    @prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "    @prefix rel: <http://www.perceive.net/schemas/relationship/> .\n",
    "\n",
    "    <#green-goblin>\n",
    "        rel:enemyOf <#spiderman> ;\n",
    "        a foaf:Person ;    # in the context of the Marvel universe\n",
    "        foaf:name \"Green Goblin\" .\n",
    "\n",
    "    <#spiderman>\n",
    "        rel:enemyOf <#green-goblin> ;\n",
    "        a foaf:Person ;\n",
    "        foaf:name \"Spiderman\", \"Человек-паук\"@ru .\n",
    "\n",
    "This example introduces many of features of the Turtle language: @base and Relative IRI references, @prefix and prefixed names, predicate lists separated by ';', object lists separated by ',', the token a, and literals. Comments may be given after a '#'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install spaCy run \"conda install -c conda-forge spacy\" and then \"python -m spacy download en_core_web_sm\" in the anaconda terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "import spacy\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download as nltk_down\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "The data consists of three documents:\n",
    "- fiction text - The Phoenix on the Sword by Robert E. Howard available via [Project Gutenberg of Australia](http://www.gutenberg.net.au/ebooks06/0600811h.html)\n",
    "- media text - 14 articles from Reuters and DW\n",
    "- enciclopedic text - around 30 Wikipedia articles from different domains: Biology, Geography, Physics, Chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that loads our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_file(file):\n",
    "    text = ''\n",
    "    with open(file, 'r', encoding='utf-8-sig') as input_file:\n",
    "        text = input_file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the text documents we can see the number of characters in each document or some regular expressions to measure the size of the documents in words. The word count should be similar for each of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_text = load_input_file('data/conan.txt')\n",
    "fiction_words = re.split(\"\\W+\", fiction_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_text = load_input_file('data/media.txt')\n",
    "media_words = re.split(\"\\W+\", media_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text = load_input_file('data/wiki.txt')\n",
    "wiki_words = re.split(\"\\W+\", wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character counts: fiction - 50253, media - 57779, wiki - 63848\n",
      "RE word counts: fiction - 9186, media - 9597, wiki - 10219\n"
     ]
    }
   ],
   "source": [
    "print(f\"Character counts: fiction - {len(fiction_text)}, media - {len(media_text)}, wiki - {len(wiki_text)}\")\n",
    "print(f\"RE word counts: fiction - {len(fiction_words)}, media - {len(media_words)}, wiki - {len(wiki_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character counts are not very informative and matching with regular expressions is prone to errors. For text processing is better to look for sentences and words. This can be done wit the help of NLTK and spaCy libraries. They can apply the processes of sentence splitting and tokenization (taking a text or sentence and splitting it into individual units called tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_tokens = word_tokenize(fiction_text)\n",
    "media_tokens = word_tokenize(media_text)\n",
    "wiki_tokens = word_tokenize(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK word counts: fiction - 10585, media - 10728, wiki - 11586'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_counts = f\"NLTK word counts: fiction - {len(fiction_tokens)}, media - {len(media_tokens)}, wiki - {len(wiki_tokens)}\"\n",
    "nltk_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the there is a difference. Maybe an arbiter will help. What will spaCy count.\n",
    "First load the model, then make the document and see the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_doc = nlp(fiction_text)\n",
    "media_doc = nlp(media_text)\n",
    "wiki_doc = nlp(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy word counts: fiction - 11041, media - 11206, wiki - 11897\n",
      "NLTK word counts: fiction - 10585, media - 10728, wiki - 11586\n"
     ]
    }
   ],
   "source": [
    "spacy_counts = f\"spaCy word counts: fiction - {len(fiction_doc)}, media - {len(media_doc)}, wiki - {len(wiki_doc)}\"\n",
    "print(spacy_counts)\n",
    "print(nltk_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem that NLTK and spaCy tokenizers are producing different token counts. Lets see the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_result = []\n",
    "for nltk_token in wiki_tokens[:20]:\n",
    "    nltk_result.append(nltk_token)\n",
    "\n",
    "spacy_result = []\n",
    "for spacy_token in wiki_doc[:20]:\n",
    "    spacy_result.append(spacy_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Turtles', 'are', 'an', 'order', 'of', 'reptiles', 'known', 'as', 'Testudines', ',', 'characterized', 'by', 'a', 'shell', 'developed', 'mainly', 'from', 'their', 'ribs', '.']\n",
      "[Turtles, are, an, order, of, reptiles, known, as, Testudines, ,, characterized, by, a, shell, developed, mainly, from, their, ribs, .]\n"
     ]
    }
   ],
   "source": [
    "print(nltk_result)\n",
    "print(spacy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks fine. Lets try with the number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_sents = sent_tokenize(fiction_text)\n",
    "media_sents = sent_tokenize(media_text)\n",
    "wiki_sents = sent_tokenize(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentences = f\"NLTK sentence counts: fiction - {len(fiction_sents)}, media - {len(media_sents)}, wiki - {len(wiki_sents)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_spacy_sent = list(fiction_doc.sents)\n",
    "media_spacy_sent = list(media_doc.sents)\n",
    "wiki_spacy_sent = list(wiki_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentences = f\"spaCy sentence counts: fiction - {len(fiction_spacy_sent)}, media - {len(media_spacy_sent)}, wiki - {len(wiki_spacy_sent)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK sentence counts: fiction - 565, media - 433, wiki - 486\n",
      "spaCy sentence counts: fiction - 527, media - 434, wiki - 485\n"
     ]
    }
   ],
   "source": [
    "print(nltk_sentences)\n",
    "print(spacy_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough. It seems that the splitters disagree most in the fiction document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "For the purposes of the experiments the text will need to be separated into sentences and tokes. Then the tokens will need to be lemmatized and recieve part-of-speech-tags.\n",
    "\n",
    "Lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item and Part-of-speech (POS) tagging is the assignment of part-of-speech tags to words.\n",
    "\n",
    "Trying to measure the size of the documents the separation work is done. NLTK does lemmatization and POS-tagging via separate methods while spaCy does all at doc creation.\n",
    "\n",
    "Lets begin with NLTK lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ivo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load once for lemmatizer to work\n",
    "nltk_down('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a function that takes the tokens and utilizes a lemmatizer from NLTK. The WordNet one is recommended, but other are available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lematize_tokens(tokens):\n",
    "    lemmas = []\n",
    "\n",
    "    for token in tokens:\n",
    "        lemmetized_word = lemmatizer.lemmatize(token)\n",
    "        lemmas.append(lemmetized_word)\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_lemmas = lematize_tokens(fiction_tokens)\n",
    "media_lemmas = lematize_tokens(media_tokens)\n",
    "wiki_lemmas = lematize_tokens(wiki_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement POS-tagging to the lemmatized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_pos = pos_tag(fiction_lemmas)\n",
    "media_pos = pos_tag(media_lemmas)\n",
    "wiki_pos = pos_tag(wiki_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HONG', 'NNP'),\n",
       " ('KONG/BEIJING', 'NNP'),\n",
       " (',', ','),\n",
       " ('Aug', 'NNP'),\n",
       " ('2', 'CD'),\n",
       " ('(', '('),\n",
       " ('Reuters', 'NNPS'),\n",
       " (')', ')'),\n",
       " ('-', ':'),\n",
       " ('Some', 'DT'),\n",
       " ('Chinese', 'JJ'),\n",
       " ('city', 'NN'),\n",
       " ('government', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('made', 'VBN'),\n",
       " ('it', 'PRP'),\n",
       " ('harder', 'JJR'),\n",
       " ('for', 'IN'),\n",
       " ('developer', 'NN'),\n",
       " ('to', 'TO')]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_pos[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should do it. Now to proceed with the triple extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Generating Triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK and spaCy behave differently so they will be implemented separately and their results will be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Triples\n",
    "\n",
    "From the text. Look for target predicates. Look for nouns before and after the predicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack the logic in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_extract_triples(pos_tokens):\n",
    "    nltk_triples = []\n",
    "\n",
    "    for idx, tup in enumerate(pos_tokens):\n",
    "        if tup[0] in ['is', 'are', 'have', 'can']:\n",
    "            predicate = tup[0]\n",
    "            subj = ''\n",
    "            obj = ''\n",
    "            \n",
    "            for i in range(idx, -1, -1):\n",
    "                if pos_tokens[i][1].startswith('NN'):\n",
    "                    subj = pos_tokens[i][0]\n",
    "                    break\n",
    "            for j in range(idx, len(pos_tokens)):\n",
    "                if pos_tokens[j][1].startswith('NN'):\n",
    "                    obj = pos_tokens[j][0]\n",
    "                    break\n",
    "\n",
    "            triple = f\"{subj} {predicate} {obj}\"\n",
    "            nltk_triples.append(triple)\n",
    "\n",
    "    return nltk_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function with texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_triples_nltk = nltk_extract_triples(fiction_pos)\n",
    "media_triples_nltk = nltk_extract_triples(media_pos)\n",
    "wiki_triples_nltk = nltk_extract_triples(wiki_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Turtles are order',\n",
       " 'turtle are group',\n",
       " 'head are living',\n",
       " 'terrapin are continent',\n",
       " 'shell are bone',\n",
       " 'part is carapace',\n",
       " 'underside is flatter',\n",
       " 'surface is scale',\n",
       " 'Turtles are temperature',\n",
       " 'environment are omnivore',\n",
       " 'turtle are reptile',\n",
       " 'Turtles have myth',\n",
       " 'specie are pet',\n",
       " 'Turtles have meat',\n",
       " 'turtle are bycatch',\n",
       " 'world are result',\n",
       " 'specie are extinction',\n",
       " 'hedgehog is mammal',\n",
       " 'Erinaceidae are specie',\n",
       " 'introduction are Australia']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(wiki_triples_nltk))\n",
    "wiki_triples_nltk[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process for spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_triples(document):\n",
    "    spacy_triples = []\n",
    "\n",
    "    for token in document:\n",
    "        if token.lemma_ in ['be', 'have', 'can']:\n",
    "            idx = token.i\n",
    "            predicate = token\n",
    "            subj = ''\n",
    "            obj = ''\n",
    "\n",
    "            for i in range(idx, -1, -1):\n",
    "                if document[i].pos_ == \"NOUN\":\n",
    "                    subj = document[i].lemma_\n",
    "                    break\n",
    "            for j in range(idx, len(document)):\n",
    "                if document[j].pos_ == \"NOUN\":\n",
    "                    obj = document[j].lemma_\n",
    "                    break\n",
    "\n",
    "            triple = f\"{subj} {predicate} {obj}\"\n",
    "            spacy_triples.append(triple)\n",
    "\n",
    "    return spacy_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_triples_spacy = spacy_extract_triples(fiction_doc)\n",
    "media_triples_spacy = spacy_extract_triples(media_doc)\n",
    "wiki_triples_spacy = spacy_extract_triples(wiki_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['turtle are order',\n",
       " 'turtle are group',\n",
       " 'head are living',\n",
       " 'terrapin are continent',\n",
       " 'shell are bone',\n",
       " 'part is carapace',\n",
       " 'underside is plastron',\n",
       " 'surface is scale',\n",
       " 'turtle are ectotherm',\n",
       " 'environment are omnivore',\n",
       " 'turtle are reptile',\n",
       " 'turtle have myth',\n",
       " 'specie are pet',\n",
       " 'turtle have meat',\n",
       " 'turtle been meat',\n",
       " 'turtle are bycatch',\n",
       " 'world are result',\n",
       " 'world being result',\n",
       " 'specie are extinction',\n",
       " 'hedgehog is mammal']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(wiki_triples_spacy))\n",
    "wiki_triples_spacy[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number and a sample of the extracted triples between NLTK and spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "['dupe have street', 'desert have heart', 'night have agent', 'agent have face', 'face have empire', 'shadow have downfall', 'task is wit', 'task are wit', 'deed is people', 'reign is people']\n",
      "333\n",
      "['rise was age', 'world was supreme', 'countenance was door', 'hand was giant', 'dupe have street', 'desert have heart', 'desert been heart', 'night have noble', 'agent have face', 'face have empire']\n"
     ]
    }
   ],
   "source": [
    "print(len(fiction_triples_nltk))\n",
    "print(fiction_triples_nltk[:10])\n",
    "print(len(fiction_triples_spacy))\n",
    "print(fiction_triples_spacy[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "['government have developer', 'move are completion', 'sale are future', 'sector is tightening', 'project can developer', 'project is developer', 'developer have crisis', 'sector have capital', 'efficiency can credit', 'developer are project']\n",
      "440\n",
      "['government have developer', 'company be move', 'move are completion', 'sale are future', 'sector is tightening', 'government has help', 'sector has year', 'sector been year', \"demand 's dilemma\", 'project can developer']\n"
     ]
    }
   ],
   "source": [
    "print(len(media_triples_nltk))\n",
    "print(media_triples_nltk[:10])\n",
    "print(len(media_triples_spacy))\n",
    "print(media_triples_spacy[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n",
      "['Turtles > are > order', 'turtle > are > group', 'head > are > living', 'terrapin > are > continent', 'shell > are > bone', 'part > is > carapace', 'underside > is > flatter', 'surface > is > scale', 'Turtles > are > temperature', 'environment > are > omnivore']\n",
      "533\n",
      "['turtle > are > order', 'turtle > are > group', 'head > are > living', 'terrapin > are > continent', 'shell > are > bone', 'part > is > carapace', 'underside > is > plastron', 'surface > is > scale', 'turtle > are > ectotherm', 'environment > are > omnivore']\n"
     ]
    }
   ],
   "source": [
    "print(len(wiki_triples_nltk))\n",
    "print(wiki_triples_nltk[:10])\n",
    "print(len(wiki_triples_spacy))\n",
    "print(wiki_triples_spacy[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison conclusion...\n",
    "\n",
    "The difference of lemmatization leads to strange behaviour of the aux verbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Triples\n",
    "\n",
    "Create a set of all nouns. Create a set of predicates. Loop for every noun with every predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_nouns_nltk(pos_tokens):\n",
    "    nouns = set()\n",
    "    for lemma, pos_tag in pos_tokens:\n",
    "        if pos_tag.startswith('NN'):\n",
    "            nouns.add(lemma.lower())\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_nouns_nltk = collect_nouns_nltk(fiction_pos)\n",
    "media_nouns_nltk = collect_nouns_nltk(media_pos)\n",
    "wiki_nouns_nltk = collect_nouns_nltk(wiki_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['yell',\n",
       " 'mass',\n",
       " 'melancholy',\n",
       " 'money-lenders',\n",
       " 'copper',\n",
       " 'south',\n",
       " 'drank',\n",
       " 'desert',\n",
       " 'black',\n",
       " 'state',\n",
       " 'shrivels',\n",
       " 'silver',\n",
       " 'rise',\n",
       " 'master',\n",
       " 'minstrel',\n",
       " 'cimmerian',\n",
       " 'court',\n",
       " 'pack',\n",
       " 'pair',\n",
       " 'hero']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(fiction_nouns_nltk))\n",
    "list(fiction_nouns_nltk)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_nouns_spacy(document):\n",
    "    nouns = set()\n",
    "    for token in document:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.add(token.lemma_)\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_nouns_spacy = collect_nouns_spacy(fiction_doc)\n",
    "media_nouns_spacy = collect_nouns_spacy(media_doc)\n",
    "wiki_nouns_spacy = collect_nouns_spacy(wiki_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cup',\n",
       " 'yell',\n",
       " 'mass',\n",
       " 'melancholy',\n",
       " 'copper',\n",
       " 'south',\n",
       " 'other',\n",
       " 'desert',\n",
       " 'state',\n",
       " 'silver',\n",
       " 'rise',\n",
       " 'master',\n",
       " 'minstrel',\n",
       " 'court',\n",
       " 'pack',\n",
       " 'wielder',\n",
       " 'pair',\n",
       " 'hero',\n",
       " 'stamp',\n",
       " 'instinct']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(fiction_nouns_spacy))\n",
    "list(fiction_nouns_spacy)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the common and the different elements in the noun sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_nouns_intersection = list(fiction_nouns_nltk & fiction_nouns_spacy)\n",
    "len(fiction_nouns_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_nouns_diff = list(fiction_nouns_nltk.difference(fiction_nouns_spacy))\n",
    "len(fiction_nouns_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yell',\n",
       " 'mass',\n",
       " 'melancholy',\n",
       " 'copper',\n",
       " 'south',\n",
       " 'desert',\n",
       " 'state',\n",
       " 'silver',\n",
       " 'rise',\n",
       " 'master',\n",
       " 'minstrel',\n",
       " 'court',\n",
       " 'pack',\n",
       " 'pair',\n",
       " 'hero',\n",
       " 'stamp',\n",
       " 'instinct',\n",
       " 'corpse',\n",
       " 'fiend',\n",
       " 'country']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_nouns_intersection[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['money-lenders',\n",
       " 'drank',\n",
       " 'shrivels',\n",
       " 'black',\n",
       " 'cimmerian',\n",
       " 'brief',\n",
       " 'primordial—legs',\n",
       " 'knew',\n",
       " 'bay',\n",
       " 'stole',\n",
       " 'breathless',\n",
       " 'stir',\n",
       " 'mazed',\n",
       " 'pallantides',\n",
       " 'undreamed',\n",
       " 'briefly',\n",
       " 'heavy',\n",
       " 'king—an',\n",
       " 'unseen',\n",
       " 'mitra']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_nouns_diff[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unite the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_nouns_all = list(fiction_nouns_nltk | fiction_nouns_spacy)\n",
    "media_nouns_all = list(media_nouns_nltk | media_nouns_spacy)\n",
    "wiki_nouns_all = list(wiki_nouns_nltk | wiki_nouns_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mass',\n",
       " 'cup',\n",
       " 'melancholy',\n",
       " 'money-lenders',\n",
       " 'copper',\n",
       " 'south',\n",
       " 'other',\n",
       " 'drank',\n",
       " 'black',\n",
       " 'state',\n",
       " 'shrivels',\n",
       " 'rise',\n",
       " 'wielder',\n",
       " 'pair',\n",
       " 'primordial—legs',\n",
       " 'bay',\n",
       " 'blossom',\n",
       " 'stole',\n",
       " 'today',\n",
       " 'stir']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_nouns_all[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same predicate set as the extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates = ['is', 'have', 'can']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triples(nouns, predicates):\n",
    "    triples = []\n",
    "    for subj in nouns:\n",
    "        for predicate in predicates:\n",
    "            for obj in nouns:\n",
    "                if subj != obj:\n",
    "                    triple = f\"{subj} {predicate} {obj}\"\n",
    "                    triples.append(triple)\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_generated_triples = generate_triples(fiction_nouns_all, predicates)\n",
    "media_generated_triples = generate_triples(media_nouns_all, predicates)\n",
    "wiki_generated_triples = generate_triples(wiki_nouns_all, predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5382780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cup is october',\n",
       " 'cup is rica',\n",
       " 'cup is april',\n",
       " 'cup is dz',\n",
       " 'cup is south',\n",
       " 'cup is other',\n",
       " 'cup is midday',\n",
       " 'cup is state',\n",
       " 'cup is parks',\n",
       " 'cup is rate',\n",
       " 'cup is coach',\n",
       " 'cup is rise',\n",
       " 'cup is garissa',\n",
       " 'cup is chief',\n",
       " 'cup is compared',\n",
       " 'cup is evolving',\n",
       " 'cup is doksuri',\n",
       " 'cup is entirety',\n",
       " 'cup is eemshaven',\n",
       " 'cup is putschists']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(media_generated_triples))\n",
    "media_generated_triples[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triple lists are slow and with millions of elements. Save them as turtle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_triples(triples, file_name):\n",
    "    with open(f'results/{file_name}.ttl', 'w') as output_file:\n",
    "        for triple in triples:\n",
    "            output_file.write(f'{triple} .\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it the first time. After that the files can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_triples(fiction_generated_triples, 'fiction_triples')\n",
    "# save_triples(media_generated_triples, 'media_triples')\n",
    "# save_triples(wiki_generated_triples, 'wiki_triples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_triples(fiction_triples_nltk, 'fiction_triples_extracted_nltk')\n",
    "save_triples(fiction_triples_spacy, 'fiction_triples_extracted_spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_triples(media_triples_nltk, 'media_triples_extracted_nltk')\n",
    "save_triples(media_triples_spacy, 'media_triples_extracted_spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_triples(wiki_triples_nltk, 'wiki_triples_extracted_nltk')\n",
    "save_triples(wiki_triples_spacy, 'wiki_triples_extracted_spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The goal is to extract real live knowledge from real live text documents.\n",
    "\n",
    "triple validation - how to filter \"wrong\" ones?\n",
    "\n",
    "Good example - \"hedgehog is mammal\"; interesting example - \"specie are extinction\"; bad example - \"world are result\".\n",
    "\n",
    "The interesting examples capture some real life knowledge, but skewed. Compare \"specie are extinction\" to \"specie can_be extinct\" or \"specie is extinct\".\n",
    "\n",
    "How to determine the members of predicate sets both for extraction and for generation.\n",
    "\n",
    "Overgeneration - the generator functions produces too much, but captures all.\n",
    "\n",
    "Comparison of productivity - it was expected the most productive text to be the wiki articles and the least productive the fiction text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [RDF](https://www.w3.org/RDF/)\n",
    "2. [RDF Primer](https://www.w3.org/TR/2014/NOTE-rdf11-primer-20140624/)\n",
    "3. [RDF Triples](https://www.w3.org/TR/rdf12-n-triples/)\n",
    "4. [Turtle Format](https://www.w3.org/TR/rdf12-turtle/)\n",
    "5. [Entity extraction: From unstructured text to DBpedia RDF triples](https://lucris.lub.lu.se/ws/portalfiles/portal/3053000/3191702.pdf)\n",
    "6. [FactCheck: Validating RDF Triples Using Textual Evidence](https://svn.aksw.org/papers/2018/CIKM_FACTCHECK/public.pdf)\n",
    "7. [Harvesting RDF triples](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=05620dd4b5346e5c17f3f4da97efcb18e2fcb7e6)\n",
    "8. [NLTK](https://www.nltk.org/)\n",
    "9. [Install spaCy](https://spacy.io/usage)\n",
    "10. [NLP with spaCy](http://spacy.pythonhumanities.com/intro.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
